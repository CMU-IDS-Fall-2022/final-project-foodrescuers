# Final Project Report

**Project URL**: TODO
**Video URL**: TODO

Short (~250 words) abstract of the concrete data science problem and how the solutions addresses the problem.

## Introduction
1.3 billion tons of food is wasted annually, worth approximately $1 trillion. Food waste occurs for a variety of reasons, including: uneaten food that is thrown out at homes and restaurants, crops left in fields because of low crop prices or too many of the same crops being available, problems during the manufacturing and transportation of food, and food not meeting retailers' standards for color and appearance (How we fight food waste in the US. Feeding America). 
According to the United States Department of Agriculture (USDA), between 30 - 40 percent of the food supply in the US is wasted. This corresponds to about 133 billion pounds and $161 billion worth of food in 2010. This is not just a problem in the United States; one-third of food produced for human consumption is lost or wasted globally. If wasted food were a country, it would be the world's third-largest producer of carbon dioxide, after the USA and China (“5 Facts about Food Waste and Hunger | World Food Programme”). We have developed an application that will allow people to make conscious decisions about the food they already have to reduce waste. We provide an interface where the user can input the ingredients they have at home, find popular recipes with those ingredients, and provide the most environmentally friendly dish they can make with their home ingredients.
Our project enables everyday consumers to better understand the scope of food waste, and how they can do their part to reduce that waste.

Note (for the grader): Due to Streamlit server issue, sometimes the “Find recipes” button might error out or take a really long time to load. Normally, it should only take about half a minute for the results to show up. There is a progress bar. If the button stopped working, reloading the site should work. Running this app offline is more reliable in our experience.

## Related Work

Data Visualizations of food waste around the world:
	One of the best sources to see food waste around the world is from the Food and Agriculture Organization (FAO) of the United Nations. The FAO aggregates data from “700 publications and reports from various sources (e.g., subnational reports, academic studies, FAOSTAT and reports from national and international organizations such as the World Bank, GIZ, FAO, IFPRI, and other sources), producing more than 50,000 data points to date” (“Food and Agriculture Organization of the United Nations”).
They provide data that can be broken down by year, geographic region and countries, commodities, and the value chain stage(s) in which the food was lost. Additionally, the FAO provides the method of data collection. This data can be viewed and downloaded on the FAO website (“Food and Agriculture Organization of the United Nations”).  In 2020, the FAO released 18,754 data points across 133 countries/regions. In November 2021, the FAO aggregated more than 29,000 data points. 
	Data visualizations from the FAO served as inspiration to our visualizations allowing users to contextualize the problem of food waste around the world. 
  
Recipe recommendation based on input ingredients:
There are several recipe recommendations. Yummly (https://www.yummly.com) is a site that recommends recipes based on user inputs such as allergy, cooking level skills, and style of dishes. This style of recommendation is quite different from what we implemented since we didn’t take into account dish style or allergy. Pantry (https://www.supercook.com/#/desktop) is another site that also recommends recipes. This site lets the user select the ingredients that they have per category of food type such as “Pantry Essentials”. This is slightly different from what we implemented in the sense that it doesn’t allow for full text entry, but more of a search and select each ingredient. Our implementation allows for ease of use since it uses a machine learning model to analyze the text and match the ingredients.
The Environmental Impacts of Food from the website (https://ourworldindata.org/environmental-impacts-of-food) has a visualization for the impact of food. However, they have a bar chart for each of the categories of impact so there ended up being 5 different bar graphs.


## Methods

Introduction:
To set up our data to make insightful visualizations, we wanted to include information about each country per year. This required some data processing of the dataset from the Food and Agriculture Organizations of the United Nations (FAO dataset). In order to visualize our data geographically, we needed to connect our data to an altair geodataset. The typical way of doing so requires a join on country ID, where the ID comes from the ISO 3166 codes. Thus, we first grouped our data by country and year and performed several aggregations including record count, loss average, and most lossy commodity. Then we cleaned the countries. We dropped year-country-commodity duplicates and took the record with the highest loss. We also had to clean the country column to match the ISO 3166 codes; for example “United States of America” became “United States”. Then we joined our data with the ISO 3166 code data before then joining that on the altair geodata.
To contextualize the impact of food loss around the world, we give an opportunity for users to input the year and country they were born, or in which they are interested. Once a user populates this information, we show the largest food loss item for that year and a map displaying the food loss average globally. Food loss is further broken down within a country by the commodity type. 
An example of this would be a user selecting that they were born in, or interested in, food loss in 2003 for the United States. The user would then be presented with information showing the top ten commodities with the highest loss percentage (percentage of a food produced that was lost) - that includes more than 40% of grapefruit, pineapple, and orange juices lost of that produced for 2003 in the US. Ideally, this gives a user an opportunity to reflect on what ingredients they have in their fridge / pantry, and what they are actually consuming. 
A user can also see the average loss percentage of food around the world for a given year. On average, their country may be better or worse than the world. We considered a few different metrics before settling on average. First we tried summing the total loss over all commodities for a given country-year pair, but this misrepresented countries that simply had more data. Thus, an average loss metric seemed most fair. 

EDA:
Data processing:
The provided dataset (Our World in Data | Environmental Impacts of Food) has multiple categories of impact: greenhouse gas emissions, land use, eutrophication, water scarcity and water withdrawals. We chose to focus on the four impact categories of greenhouse gas emissions, land use, eutrophication, and water withdrawals. One of the issues we faced was that these categories each have different units and so when we overlay them through a stacked bar chart, not all of the categories show up equally. To overcome this, we create columns for the normalized value of the impact. For example, each greenhouse gas emission of an ingredient will be divided by the highest emission in an ingredient. This ratio is then summed across the four categories and normalized to a scale of 100 to create what we call the impact index. This index is what we used to rank the impact of each of the ingredients, for ease of comparison between the food products.
To contextualize the food impact of the recipes, we provided the user some metrics for their choices of recipes if they switch from the recipe with the highest impact index to lowest impact from the recommended recipes. To contextualize the impact, we quantify the amount of saving through concrete context. For example, for land use, we find the difference in land use impact between the two recipes, then divide this difference by the number of basketball courts area and multiply this by 365 to find the amount of basketball courts saved, assuming if the user makes this change in recipe every day for a year. The links for these metrics are provided on the application.

Interface techniques:
To visualize the impact of the ingredients, we use multi-view coordination between the stacked bar chart and a treemap. This multi-view is between the Plotly treemap component and the Altair stacked bar chart component. Because of the differences in library, multi-view capability needed an additional plug in for it to work since Altair brush selection is purely a front-end event. The plug in captures the select event of the user and equates that to the data the user selected. The selected data is passed out as a click event dictionary from the Altair chart. This event dictionary is then used to filter the dataset with the user inputs and be fed into the Plotly treemap. The stacked bar chart shows the aggregate impact of a food ingredient, broken down by the category of impact. This stacked bar chart is a multi-select bar chart, where there are color highlights for the selected bar. When the bars are selected, the tree map would adjust to show the impact value of the selected commodities and be displayed. This way the user can easily compare between commodities both the aggregate impact, and within each impact category.

ML:
We used one of the datasets that can be found in the Data Explorer: Environmental Impacts of Food as one of our sources. After downloading the data, we were presented with six datasets. These datasets were later combined into one dataset and used as the basis for the main impact visualization that was developed using that dataset. Unfortunately, we could not use this dataset to calculate grams. We searched the internet for a dataset in which grams could be calculated, but most of the datasets we found did not include any columns for fruits or measurements. A number of datasets also had a very small number of fruit entries in their fruit column, and this was a serious problem. Having scoured the internet for quite a while to find the right data, we finally decided to combine two datasets: food and food_portion, in order to achieve our goal. It is important to note that the first dataset had ample amounts of fruit values, whereas the second dataset had measurements of fruits. The inner join was used to join the datasets based on their ID column, and following this, we had to do several data cleaning such as removing duplicates and missing values from the dataset. It was this process that enabled us to produce a useful dataset which allowed us to calculate the gram.

Our goal with this project was to get the best recipes and the most relevant recipes and their impact on the environment for a particular user. There are many existing recipe recommendation systems online or as independent projects. But we approached this task with a perspective to reduce the impact on the environment. Along with the recipe, we also calculated the impact the recipe can cause on the environment with different factors such as carbon emissions, water use, land use, etc. We then visualized these factors and the comparison of the recipes on our website. 
We firstly needed to find the dataset for recipe recommendation such that we could make use of it and recommend the best recipe for a user’s requirements. The food.com recipe dataset on kaggle is a large collection of recipes and their respective attributes such as the ingredients, their nutritional value, the instructions, the time it takes to cook, etc. This dataset was well aligned with our project because of its simplicity of use in a CSV file. The actual dataset contains more than 230 thousand recipes and rich attributes for all the recipes. To use this dataset for our project, we determined that using the ingredients would give us the most accurate results for a recipe. 
We take in a list of ingredients available to a particular user and try to find the recipes in a dataset that contains similar ingredients. Rather than asking the user to think about what they should eat, we make the task easier for the user by just asking them the ingredients in their kitchen or refrigerator. The main challenge with this task was to find the closest matches of a recipe given the ingredients. This is a text matching task that can be dealt with using many methods such as using large language models or just trying to see which recipe has all the ingredients we have. We realized that this is not feasible as the ingredients may differ in their names drastically such as different types of cheeses. 
To overcome this, we used the TF-IDF technique. TF-IDF uses the frequency of a word in a document as well as the frequency of that word occurring in all the documents to encode the document. Initially, we convert the ingredient list in the dataset to a raw string without any commas and preprocess it such that it removes the unnecessary closing and opening square brackets (‘[‘ and ‘]’) . We store these TF-IDF encodings for every recipe’s ingredients so that we can use it later when an input of ingredients is provided. Subsequently, when a user provides the ingredients in their inventory, we find its TFIDF encoding and then use cosine similarity to find the recipes that have the most similar ingredients. We find the top 5 recipes that have the most cosine similarity amongst all the recipes. 
Once we have the recipes, for every recipe, we calculate the impact it can cause on the environment in terms of the same factors provided in the main dataset. We face some issues in this approach. Particularly, the unavailability of the portion sizes of each ingredient in that recipe. The impact metrics available in the main dataset are per kilograms of that item. Thus, to overcome this, we used a different dataset from the U.S. Department of Agriculture that contained the information about the different portion sizes of various types of items available in stores. This contains items such as a particular brand’s hummus as well as regular hummus. We realized that using this dataset would be quite helpful for this task as we will be able to take in not just generic names of ingredients but also of various brands. Once we get the standard portion size of an ingredient, we just scale the impact of that item from the original dataset i.e. 10 grams of bacon will have 1/100th of the impact as the one caused by a kilogram of bacon. Once we get the scaled impact, we just add together the impact for the standard portions of every ingredient. 
Additionally, another important aspect of the recipe recommendation and impact calculation is the Levenshtein distance calculation for string matching. This is a string matching metric that calculates the number of updates to a string to convert it to the second string. Two exact same strings will have a Levenshtein distance of 0 whereas two strings such as apples and green apples will have a distance of 6. We use this to find the closest matching item in our datasets given an ingredient. This also helps us map the ingredients in the food portion dataset to the food impact dataset.



## Results

## Discussion

## Future Work
